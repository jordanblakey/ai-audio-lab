<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recorder</title>
    <style>
        #streamInfo {
            display: flex;
            flex-direction: column;
            flex-wrap: wrap;
            max-height: 80vh;
        }
    </style>
</head>
<body>
    <h1>gRPC Whisper Transcription Client</h1>
    <h3><a href="/">Transcription</a> <a href="/recorder">Recorder</a></h3>
    <button id="start">Start</button>
    <button id="stop">Stop</button>
    <pre id="mediaRecorderInfo"></pre>
    <div id="streamInfo"></div>
    <div id="waveform-container" style="position: fixed; bottom: 0; left: 0; width: 100%; height: 100px; background: #333; z-index: 1000;">
        <canvas id="waveform" style="width: 100%; height: 100%; display: block;"></canvas>
    </div>
    <script>
        const startButton = document.getElementById('start');
        const stopButton = document.getElementById('stop');
        let mediaRecorder;
        let stream;
        let streamInfoRefreshInterval;
        let recordingStartTime;
        const audioChunks = [];
        const audioPeaks = []; // Store max amplitude per frame
        const waveformCanvas = document.getElementById('waveform');
        const waveformCtx = waveformCanvas.getContext('2d');
        let audio;
        
        // Handle High DPI displays
        function resizeWaveform() {
            const dpr = window.devicePixelRatio || 1;
            const rect = waveformCanvas.getBoundingClientRect();
            waveformCanvas.width = rect.width * dpr;
            waveformCanvas.height = rect.height * dpr;
            waveformCtx.scale(dpr, dpr);
        }
        window.addEventListener('resize', resizeWaveform);
        resizeWaveform();


        startButton.addEventListener('click', async () => {
            console.log('Start button clicked!');
            if (streamInfoRefreshInterval) {
                clearInterval(streamInfoRefreshInterval);
            }
            if (audio instanceof Audio) {
                audio.pause();
            }
            const constraints = {
                audio: {
                    echoCancellation: true,
                    noiseSuppression: false,
                    autoGainControl: true,
                    sampleRate: 16000,
                }
            }
            const stream = await navigator.mediaDevices.getUserMedia(constraints);
            console.log(stream);
            recordingStartTime = Date.now();
            mediaRecorder = new MediaRecorder(stream);

            mediaRecorder.ondataavailable = event => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = () => {
                const mimeType = mediaRecorder.mimeType || 'audio/webm';
                console.log(`Recorder stopped. MIME type: ${mimeType}`);
                console.log(`Total chunks: ${audioChunks.length}`);
                
                if (audioChunks.length === 0) {
                    console.error('No audio chunks recorded.');
                    return;
                }

                const audioBlob = new Blob(audioChunks, { type: mimeType });
                const audioUrl = URL.createObjectURL(audioBlob);

                console.log('Audio Blob URL:', audioUrl);
                
                audio = new Audio();
                audio.src = audioUrl;
                audio.oncanplaythrough = () => {
                    console.log('Audio can play through');
                    audio.play().catch(e => console.error('Play error:', e));
                };
                audio.onerror = (e) => {
                    console.error('Audio error:', e);
                    console.error('Audio error code:', audio.error ? audio.error.code : 'unknown');
                    console.error('Audio network state:', audio.networkState);
                };
                
                audioChunks.length = 0;
            };

            console.log(mediaRecorder);
            // Pass a timeslice (e.g., 1000ms) to fire dataavailable periodically
            // mediaRecorder.start(1000); 
            mediaRecorder.start(15);
            audioPeaks.length = 0; // Reset waveform 
            console.log(audioChunks)
            
            // Add volume monitoring
            const audioContext = new AudioContext();
            const source = audioContext.createMediaStreamSource(stream);
            const analyzer = audioContext.createAnalyser();
            analyzer.fftSize = 256;
            source.connect(analyzer);
            
            renderStreamInfo(stream, mediaRecorder, analyzer);
            streamInfoRefreshInterval = setInterval(() => {
                renderStreamInfo(stream, mediaRecorder, analyzer);
            }, 15);
        });

        function renderStreamInfo(stream, mediaRecorder, analyzer) {
            const streamInfo = document.getElementById('streamInfo');
            streamInfo.replaceChildren()
            const tracks = stream.getAudioTracks()
            const track = tracks[0]
            // console.log(track)
            
            // Check volume level
            const dataArray = new Uint8Array(analyzer.frequencyBinCount);
            analyzer.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((prev, curr) => prev + curr, 0) / dataArray.length;
            const EstimatedSoundPressureLevelDb = average + 33;
            
            // --- Waveform Data Collection ---
            const timeData = new Uint8Array(analyzer.fftSize);
            analyzer.getByteTimeDomainData(timeData);
            let max = 0;
            for(let i = 0; i < timeData.length; i++) {
                const v = Math.abs(timeData[i] - 128) / 128; // Normalize 0-1
                if (v > max) max = v;
            }
            audioPeaks.push(max);
            drawWaveform();
            // --------------------------------

            let misc = document.createElement('pre')
            misc.textContent += `recordingDuration: ${((Date.now() - recordingStartTime) / 1000).toFixed(2)}s\n`
            misc.textContent += `EstimatedSoundPressureLevelDb: ${(EstimatedSoundPressureLevelDb).toFixed(2)}\n`
            misc.textContent += `track.id: ${track.id}\n`
            misc.textContent += `track.kind: ${track.kind}\n`
            misc.textContent += `track.label: ${track.label}\n`
            misc.textContent += `track.muted: ${track.muted}\n`
            misc.textContent += `track.enabled: ${track.enabled}\n`
            streamInfo.appendChild(misc);

            // let capabilities = document.createElement('pre')
            // capabilities.textContent = JSON.stringify(track.getCapabilities(), null, 2);
            // streamInfo.appendChild(capabilities);

            let settings = document.createElement('pre')
            settings.textContent += `settings:\n`
            settings.textContent += JSON.stringify(track.getSettings(), null, 2);
            streamInfo.appendChild(settings);

            let constraints = document.createElement('pre')
            constraints.textContent += `constraints:\n`
            constraints.textContent += JSON.stringify(track.getConstraints(), null, 2);
            streamInfo.appendChild(constraints);

            let stats = document.createElement('pre')
            stats.textContent += `stats:\n`
            stats.textContent += JSON.stringify(track.stats, null, 2);
            streamInfo.appendChild(stats);

            let analyzerInfo = document.createElement('pre')
            analyzerInfo.textContent += `analyzer:\n`
            analyzerInfo.textContent += `analyzer.frequencyBinCount: ${analyzer.frequencyBinCount}\n`
            analyzerInfo.textContent += `analyzer.fftSize: ${analyzer.fftSize}\n`
            analyzerInfo.textContent += `analyzer.maxDecibels: ${analyzer.maxDecibels}\n`
            analyzerInfo.textContent += `analyzer.minDecibels: ${analyzer.minDecibels}\n`
            analyzerInfo.textContent += `analyzer.smoothingTimeConstant: ${analyzer.smoothingTimeConstant}\n`
            analyzerInfo.textContent += `analyzer.channelInterpretation: ${analyzer.channelInterpretation}\n`
            
            // console.log(analyzer)
            
            // analyzerInfo.textContent += JSON.stringify(analyzer, null, 2);
            streamInfo.appendChild(analyzerInfo);
        }

        function drawWaveform() {
            const width = waveformCanvas.width / (window.devicePixelRatio || 1);
            const height = waveformCanvas.height / (window.devicePixelRatio || 1);
            
            waveformCtx.clearRect(0, 0, width, height);
            waveformCtx.fillStyle = '#0f0'; // bright green
            
            const totalPeaks = audioPeaks.length;
            if (totalPeaks === 0) return;
            
            // If we have more points than pixels, we downsample visually (take MAX of the chunk)
            // ensuring we never draw more bars than there are horizontal pixels.
            // This keeps performance O(W) instead of O(N).
            
            if (totalPeaks <= width) {
                const barWidth = width / totalPeaks;
                waveformCtx.beginPath();
                for (let i = 0; i < totalPeaks; i++) {
                    const x = i * barWidth;
                    const amp = audioPeaks[i];
                    const barHeight = amp * height;
                    const y = (height - barHeight) / 2;
                    waveformCtx.fillRect(x, y, Math.max(1, barWidth - 0.5), barHeight); 
                }
            } else {
                // Downsample mode
                const samplesPerPixel = totalPeaks / width;
                waveformCtx.beginPath();
                for (let x = 0; x < width; x++) {
                    // Find max amp in this pixel's time range
                    const startIdx = Math.floor(x * samplesPerPixel);
                    const endIdx = Math.floor((x + 1) * samplesPerPixel);
                    
                    let maxAmp = 0;
                    for(let j = startIdx; j < endIdx && j < totalPeaks; j++) {
                        if (audioPeaks[j] > maxAmp) maxAmp = audioPeaks[j];
                    }
                    
                    const barHeight = maxAmp * height;
                    const y = (height - barHeight) / 2;
                    // Draw 1px wide bar
                    waveformCtx.fillRect(x, y, 1, barHeight);
                }
            }
        }

        stopButton.addEventListener('click', () => {
            console.log('Stop button clicked');
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            if (streamInfoRefreshInterval) {
                clearInterval(streamInfoRefreshInterval);
            }
        });
    </script>
</body>
</html>